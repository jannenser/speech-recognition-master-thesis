{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDttqNRmz5EH",
        "outputId": "b454669b-d7be-4cb1-b767-ebd91adee0fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.33.3-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.2/532.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.3.3)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt>=0.8.0 (from comet_ml)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.27.1)\n",
            "Collecting semantic-version>=2.8.0 (from comet_ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting sentry-sdk>=1.1.0 (from comet_ml)\n",
            "  Downloading sentry_sdk-1.25.0-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson (from comet_ml)\n",
            "  Downloading simplejson-3.19.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.26.15)\n",
            "Collecting websocket-client<1.4.0,>=0.55.0 (from comet_ml)\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.14.1)\n",
            "Collecting wurlitzer>=1.0.2 (from comet_ml)\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Collecting everett[ini]<3.2.0,>=1.0.1 (from comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.21.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.1/510.1 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.3.4)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.19.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Installing collected packages: everett, wurlitzer, websocket-client, simplejson, sentry-sdk, semantic-version, python-box, dulwich, configobj, requests-toolbelt, comet_ml\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.5.1\n",
            "    Uninstalling websocket-client-1.5.1:\n",
            "      Successfully uninstalled websocket-client-1.5.1\n",
            "Successfully installed comet_ml-3.33.3 configobj-5.0.8 dulwich-0.21.5 everett-3.1.0 python-box-6.1.0 requests-toolbelt-1.0.0 semantic-version-2.10.0 sentry-sdk-1.25.0 simplejson-3.19.1 websocket-client-1.3.3 wurlitzer-3.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install comet_ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTy6RVBsU8-u"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import os\n",
        "from typing import Tuple, Union\n",
        "from pathlib import Path\n",
        "\n",
        "import torchaudio\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as data\n",
        "\n",
        "import os\n",
        "from comet_ml import Experiment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQWML5zLXu5E"
      },
      "source": [
        "Прикрепляемся к нашему гугл диску, где в корне лежит AudioDiploma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmCUqEQ9L4hB",
        "outputId": "6695e70a-02ac-4811-b767-8e21ae5a4bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qw8Gjx0X4Nw"
      },
      "source": [
        "# Data preparation from google drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/AudioDiploma/swiss.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "\n",
        "path_bern = '/content/drive/MyDrive/AudioDiploma/clear_bern_test.csv'\n",
        "df_bern = pd.read_csv(path_bern)\n",
        "\n",
        "df_bern = df_bern.rename(columns={'path': 'filename', 'swiss translation': 'text'})\n",
        "df_bern = df_bern.drop(['sentence'], axis=1)\n",
        "\n",
        "\n",
        "path_bern_train = '/content/drive/MyDrive/AudioDiploma/clear_bern_train36.csv'\n",
        "df_bern_train = pd.read_csv(path_bern_train)\n",
        "\n",
        "df_bern_train = df_bern_train.rename(columns={'path': 'filename', 'swiss': 'text'})\n",
        "df_bern_train = df_bern_train.drop(['sentence'], axis=1)\n",
        "\n",
        "df_s = [df, df_bern, df_bern_train]\n",
        "df.shape[0], df_bern.shape[0], df_bern_train.shape[0]"
      ],
      "metadata": {
        "id": "W4E9jLTNHeUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_path(x):\n",
        "  '''allocation of corresponding audiofile path'''\n",
        "  tmp_mark = ''\n",
        "\n",
        "  if '1235' in x:\n",
        "    tmp_mark = '1235'\n",
        "  elif '1240' in x:\n",
        "    tmp_mark = '1240'\n",
        "  elif '.flac' in x:\n",
        "    tmp_mark = 'corpus2test'\n",
        "\n",
        "  default_path = '/content/drive/MyDrive/AudioDiploma/'\n",
        "  mark = (str(tmp_mark) + '/') if tmp_mark != '' else ''\n",
        "\n",
        "  return default_path + mark + x\n",
        "\n",
        "\n",
        "def get_path_big(x):\n",
        "  tmp_mark = 'clips'\n",
        "\n",
        "  default_path = '/content/drive/MyDrive/AudioDiploma/'\n",
        "  mark = (str(tmp_mark) + '/') if tmp_mark != '' else ''\n",
        "\n",
        "  return default_path + mark + x"
      ],
      "metadata": {
        "id": "CF0IbiVJHiWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove punctuation"
      ],
      "metadata": {
        "id": "e6MJJ0RmH000"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
        "df_bern['text'] = df_bern['text'].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
        "df_bern_train['text'] = df_bern_train['text'].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
        "\n",
        "df['path'] = df.filename.apply(get_path)\n",
        "df_bern['path'] = df_bern.filename.apply(get_path)\n",
        "df_bern_train['path'] = df_bern_train.filename.apply(get_path_big)"
      ],
      "metadata": {
        "id": "PhaXO03AHmIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(df_s)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "ygTLF9ReHrkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "u4KAw94LxWuN",
        "outputId": "17b61117-5381-401e-f505-54b204778d95"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9dc32370-4b01-49bf-9fd6-6d4d3c9090a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>filename</th>\n",
              "      <th>text</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31104</td>\n",
              "      <td>22795.flac</td>\n",
              "      <td>mir daenke es isch wichtig dass mir aendlich k...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/2279...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28980</td>\n",
              "      <td>9253.flac</td>\n",
              "      <td>das isch zuelaessig brucht aber e gsetzlichi g...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/9253...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12990</td>\n",
              "      <td>34170.flac</td>\n",
              "      <td>das entspricht vierzehn prozent vo de arbeiten...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/3417...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25138</td>\n",
              "      <td>26123.flac</td>\n",
              "      <td>es aenderet nuet a der tatsach dass me sanieri...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/2612...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3185</td>\n",
              "      <td>27634.flac</td>\n",
              "      <td>weis nid obs immerno so isch</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/2763...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10838</td>\n",
              "      <td>18506.flac</td>\n",
              "      <td>es bispiu so lang mir durchschnittlech dopplet...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/1850...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7545</td>\n",
              "      <td>17727.flac</td>\n",
              "      <td>denn arbeitslosigkeit macht psychisch u physis...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/1772...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3317</td>\n",
              "      <td>29461.flac</td>\n",
              "      <td>drum isch itz entweder e praxisaenderig fuer d...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/2946...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>25681</td>\n",
              "      <td>33146.flac</td>\n",
              "      <td>me merkts dr eint oder anger spraecher vo de g...</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/3314...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>33256</td>\n",
              "      <td>36770.flac</td>\n",
              "      <td>di date blibe bi de stueuerbehoerde</td>\n",
              "      <td>/content/drive/MyDrive/AudioDiploma/clips/3677...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9dc32370-4b01-49bf-9fd6-6d4d3c9090a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9dc32370-4b01-49bf-9fd6-6d4d3c9090a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9dc32370-4b01-49bf-9fd6-6d4d3c9090a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index    filename                                               text  \\\n",
              "0  31104  22795.flac  mir daenke es isch wichtig dass mir aendlich k...   \n",
              "1  28980   9253.flac  das isch zuelaessig brucht aber e gsetzlichi g...   \n",
              "2  12990  34170.flac  das entspricht vierzehn prozent vo de arbeiten...   \n",
              "3  25138  26123.flac  es aenderet nuet a der tatsach dass me sanieri...   \n",
              "4   3185  27634.flac                       weis nid obs immerno so isch   \n",
              "5  10838  18506.flac  es bispiu so lang mir durchschnittlech dopplet...   \n",
              "6   7545  17727.flac  denn arbeitslosigkeit macht psychisch u physis...   \n",
              "7   3317  29461.flac  drum isch itz entweder e praxisaenderig fuer d...   \n",
              "8  25681  33146.flac  me merkts dr eint oder anger spraecher vo de g...   \n",
              "9  33256  36770.flac                di date blibe bi de stueuerbehoerde   \n",
              "\n",
              "                                                path  \n",
              "0  /content/drive/MyDrive/AudioDiploma/clips/2279...  \n",
              "1  /content/drive/MyDrive/AudioDiploma/clips/9253...  \n",
              "2  /content/drive/MyDrive/AudioDiploma/clips/3417...  \n",
              "3  /content/drive/MyDrive/AudioDiploma/clips/2612...  \n",
              "4  /content/drive/MyDrive/AudioDiploma/clips/2763...  \n",
              "5  /content/drive/MyDrive/AudioDiploma/clips/1850...  \n",
              "6  /content/drive/MyDrive/AudioDiploma/clips/1772...  \n",
              "7  /content/drive/MyDrive/AudioDiploma/clips/2946...  \n",
              "8  /content/drive/MyDrive/AudioDiploma/clips/3314...  \n",
              "9  /content/drive/MyDrive/AudioDiploma/clips/3677...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = df[:34000]\n",
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = df[34000:]"
      ],
      "metadata": {
        "id": "QRlaexznIEIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA4IcJIqxb0J"
      },
      "source": [
        "#Text similarity metrics and encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9ahI0GKe1vz"
      },
      "outputs": [],
      "source": [
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
        "    between two sequences. Informally, the levenshtein disctance is defined as\n",
        "    the minimum number of single-character edits (substitutions, insertions or\n",
        "    deletions) required to change one word into the other. We can naturally\n",
        "    extend the edits to word level when calculate levenshtein disctance for\n",
        "    two sentences.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in word-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Levenshtein distance and word number of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in char-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Levenshtein distance and length of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
        "    hypothesis text in word-level. WER is defined as:\n",
        "    .. math::\n",
        "        WER = (Sw + Dw + Iw) / Nw\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sw is the number of words subsituted,\n",
        "        Dw is the number of words deleted,\n",
        "        Iw is the number of words inserted,\n",
        "        Nw is the number of words in the reference\n",
        "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
        "    that empty items will be removed when splitting sentences by delimiter.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Word error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If word number of reference is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
        "    hypothesis text in char-level. CER is defined as:\n",
        "    .. math::\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
        "    encoded to unicode. Please draw an attention that the leading and tailing\n",
        "    space characters will be truncated and multiple consecutive space\n",
        "    characters in a sentence will be replaced by one space character.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Character error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If the reference length is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al34Syb-xz7A"
      },
      "source": [
        "# Audio preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyZoMPclxzGL",
        "outputId": "aeb6f829-20e4-41d2-badd-f3a329b9788d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Ou2J6Lzoou"
      },
      "source": [
        "## Кастомный librispeech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oNXdFK2zn1Q"
      },
      "outputs": [],
      "source": [
        "def load_librispeech_item_custom(item) -> Tuple[Tensor, int, str, int, int, int]:\n",
        "\n",
        "  #Load audio\n",
        "  waveform, sample_rate = torchaudio.load(item.path)\n",
        "  transcript = item.text\n",
        "\n",
        "  return (\n",
        "        waveform,\n",
        "        0,\n",
        "        transcript,\n",
        "        0,\n",
        "        0,\n",
        "        0,)\n",
        "\n",
        "class LIBRISPEECH_CUSTOM(Dataset):\n",
        "\n",
        "    def __init__(self, df) -> None:\n",
        "        self._df = df\n",
        "\n",
        "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, int, int, int]:\n",
        "        \"\"\"Load the n-th sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            n (int): The index of the sample to be loaded\n",
        "\n",
        "        Returns:\n",
        "            (Tensor, int, str, int, int, int):\n",
        "            ``(waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id)``\n",
        "        \"\"\"\n",
        "        fileid = df.iloc[n]\n",
        "        return load_librispeech_item_custom(fileid)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._df.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJu-wNhNyBd7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_SwbkpRauje"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqfYfxRnaeNu"
      },
      "outputs": [],
      "source": [
        "from comet_ml import Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJoOEWkYeoHt"
      },
      "outputs": [],
      "source": [
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNN"
      ],
      "metadata": {
        "id": "XaguorgtIUfi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjfbcsFxwYOm"
      },
      "outputs": [],
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIPPxECHwfBD"
      },
      "outputs": [],
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    with experiment.train():\n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "\n",
        "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
        "            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            iter_meter.step()\n",
        "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    best_loss = 10000\n",
        "    test_cer, test_wer = [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data\n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/AudioDiploma/own/best_model_27.pth')\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
        "    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
        "    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "    # model = SpeechRecognitionModel(\n",
        "    #     hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "    #     hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "    #     ).to(device)\n",
        "\n",
        "    # print(model)\n",
        "    # print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    # optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    # criterion = nn.CTCLoss(blank=28).to(device)\n",
        "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "    #                                         steps_per_epoch=int(len(train_loader)),\n",
        "    #                                         epochs=hparams['epochs'],\n",
        "    #                                         anneal_strategy='linear')\n",
        "\n",
        "    # iter_meter = IterMeter()\n",
        "    # for epoch in range(1, epochs + 1):\n",
        "    #     train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
        "    #     test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-WSwrmIOIYT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN0QC5etd0Cx",
        "outputId": "f9b0c4e6-6e69-4566-fdd7-ab30fb7dd101"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[-0.0388, -0.0365, -0.0345,  ..., -0.0170, -0.0187, -0.0198]]), 0, 'bi punkt zwei moechte o mir sehr unterstriiche dass dr regierigsrat z raecht druf hiiwiist d verantwortig fuer die investitione lig vollumfaenglech bim verwautigsrat vom inselspitau', 0, 0, 0)\n",
            "DataParallel(\n",
            "  (module): SpeechRecognitionModel(\n",
            "    (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (rescnn_layers): Sequential(\n",
            "      (0): ResidualCNN(\n",
            "        (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "        (layer_norm1): CNNLayerNorm(\n",
            "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (layer_norm2): CNNLayerNorm(\n",
            "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualCNN(\n",
            "        (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "        (layer_norm1): CNNLayerNorm(\n",
            "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (layer_norm2): CNNLayerNorm(\n",
            "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): ResidualCNN(\n",
            "        (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "        (layer_norm1): CNNLayerNorm(\n",
            "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (layer_norm2): CNNLayerNorm(\n",
            "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (birnn_layers): Sequential(\n",
            "      (0): BidirectionalGRU(\n",
            "        (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (1): BidirectionalGRU(\n",
            "        (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (2): BidirectionalGRU(\n",
            "        (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (3): BidirectionalGRU(\n",
            "        (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (4): BidirectionalGRU(\n",
            "        (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (classifier): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "      (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 23705373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:1699: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/24000 (0%)]\tLoss: 21.751999\n",
            "Train Epoch: 1 [200/24000 (1%)]\tLoss: 3.302601\n",
            "Train Epoch: 1 [400/24000 (2%)]\tLoss: 3.055401\n",
            "Train Epoch: 1 [600/24000 (2%)]\tLoss: 2.917023\n",
            "Train Epoch: 1 [800/24000 (3%)]\tLoss: 2.891742\n",
            "Train Epoch: 1 [1000/24000 (4%)]\tLoss: 2.961284\n",
            "Train Epoch: 1 [1200/24000 (5%)]\tLoss: 2.952481\n",
            "Train Epoch: 1 [1400/24000 (6%)]\tLoss: 3.032712\n",
            "Train Epoch: 1 [1600/24000 (7%)]\tLoss: 2.915596\n",
            "Train Epoch: 1 [1800/24000 (8%)]\tLoss: 2.973219\n",
            "Train Epoch: 1 [2000/24000 (8%)]\tLoss: 2.987431\n",
            "Train Epoch: 1 [2200/24000 (9%)]\tLoss: 2.959029\n",
            "Train Epoch: 1 [2400/24000 (10%)]\tLoss: 2.960207\n",
            "Train Epoch: 1 [2600/24000 (11%)]\tLoss: 2.907010\n",
            "Train Epoch: 1 [2800/24000 (12%)]\tLoss: 3.010152\n",
            "Train Epoch: 1 [3000/24000 (12%)]\tLoss: 2.919486\n",
            "Train Epoch: 1 [3200/24000 (13%)]\tLoss: 2.942658\n",
            "Train Epoch: 1 [3400/24000 (14%)]\tLoss: 3.040490\n",
            "Train Epoch: 1 [3600/24000 (15%)]\tLoss: 3.065488\n",
            "Train Epoch: 1 [3800/24000 (16%)]\tLoss: 2.849812\n",
            "Train Epoch: 1 [4000/24000 (17%)]\tLoss: 2.878555\n",
            "Train Epoch: 1 [4200/24000 (18%)]\tLoss: 2.848447\n",
            "Train Epoch: 1 [4400/24000 (18%)]\tLoss: 2.955796\n",
            "Train Epoch: 1 [4600/24000 (19%)]\tLoss: 2.871542\n",
            "Train Epoch: 1 [4800/24000 (20%)]\tLoss: 2.953025\n",
            "Train Epoch: 1 [5000/24000 (21%)]\tLoss: 2.878603\n",
            "Train Epoch: 1 [5200/24000 (22%)]\tLoss: 2.965466\n",
            "Train Epoch: 1 [5400/24000 (22%)]\tLoss: 2.937886\n",
            "Train Epoch: 1 [5600/24000 (23%)]\tLoss: 2.959900\n",
            "Train Epoch: 1 [5800/24000 (24%)]\tLoss: 2.819136\n",
            "Train Epoch: 1 [6000/24000 (25%)]\tLoss: 3.033116\n",
            "Train Epoch: 1 [6200/24000 (26%)]\tLoss: 2.898867\n",
            "Train Epoch: 1 [6400/24000 (27%)]\tLoss: 3.087492\n",
            "Train Epoch: 1 [6600/24000 (28%)]\tLoss: 2.937823\n",
            "Train Epoch: 1 [6800/24000 (28%)]\tLoss: 2.973952\n",
            "Train Epoch: 1 [7000/24000 (29%)]\tLoss: 2.855723\n",
            "Train Epoch: 1 [7200/24000 (30%)]\tLoss: 2.934832\n",
            "Train Epoch: 1 [7400/24000 (31%)]\tLoss: 2.807482\n",
            "Train Epoch: 1 [7600/24000 (32%)]\tLoss: 2.987558\n",
            "Train Epoch: 1 [7800/24000 (32%)]\tLoss: 2.829219\n",
            "Train Epoch: 1 [8000/24000 (33%)]\tLoss: 2.958027\n",
            "Train Epoch: 1 [8200/24000 (34%)]\tLoss: 2.886791\n",
            "Train Epoch: 1 [8400/24000 (35%)]\tLoss: 2.956069\n",
            "Train Epoch: 1 [8600/24000 (36%)]\tLoss: 2.863623\n",
            "Train Epoch: 1 [8800/24000 (37%)]\tLoss: 2.905370\n",
            "Train Epoch: 1 [9000/24000 (38%)]\tLoss: 2.824909\n",
            "Train Epoch: 1 [9200/24000 (38%)]\tLoss: 2.835230\n",
            "Train Epoch: 1 [9400/24000 (39%)]\tLoss: 2.876251\n",
            "Train Epoch: 1 [9600/24000 (40%)]\tLoss: 2.900124\n",
            "Train Epoch: 1 [9800/24000 (41%)]\tLoss: 3.028474\n",
            "Train Epoch: 1 [10000/24000 (42%)]\tLoss: 2.727782\n",
            "Train Epoch: 1 [10200/24000 (42%)]\tLoss: 2.881698\n",
            "Train Epoch: 1 [10400/24000 (43%)]\tLoss: 2.983508\n",
            "Train Epoch: 1 [10600/24000 (44%)]\tLoss: 2.634354\n",
            "Train Epoch: 1 [10800/24000 (45%)]\tLoss: 2.731427\n",
            "Train Epoch: 1 [11000/24000 (46%)]\tLoss: 2.693383\n",
            "Train Epoch: 1 [11200/24000 (47%)]\tLoss: 2.654422\n",
            "Train Epoch: 1 [11400/24000 (48%)]\tLoss: 2.598882\n",
            "Train Epoch: 1 [11600/24000 (48%)]\tLoss: 2.786842\n",
            "Train Epoch: 1 [11800/24000 (49%)]\tLoss: 2.819122\n",
            "Train Epoch: 1 [12000/24000 (50%)]\tLoss: 2.511225\n",
            "Train Epoch: 1 [12200/24000 (51%)]\tLoss: 2.767175\n",
            "Train Epoch: 1 [12400/24000 (52%)]\tLoss: 2.611917\n",
            "Train Epoch: 1 [12600/24000 (52%)]\tLoss: 2.633793\n",
            "Train Epoch: 1 [12800/24000 (53%)]\tLoss: 2.445354\n",
            "Train Epoch: 1 [13000/24000 (54%)]\tLoss: 2.372549\n",
            "Train Epoch: 1 [13200/24000 (55%)]\tLoss: 2.792820\n",
            "Train Epoch: 1 [13400/24000 (56%)]\tLoss: 2.512493\n",
            "Train Epoch: 1 [13600/24000 (57%)]\tLoss: 2.621082\n",
            "Train Epoch: 1 [13800/24000 (58%)]\tLoss: 2.318781\n",
            "Train Epoch: 1 [14000/24000 (58%)]\tLoss: 2.334622\n",
            "Train Epoch: 1 [14200/24000 (59%)]\tLoss: 2.415298\n",
            "Train Epoch: 1 [14400/24000 (60%)]\tLoss: 2.373837\n",
            "Train Epoch: 1 [14600/24000 (61%)]\tLoss: 2.502939\n",
            "Train Epoch: 1 [14800/24000 (62%)]\tLoss: 2.665023\n",
            "Train Epoch: 1 [15000/24000 (62%)]\tLoss: 2.455517\n",
            "Train Epoch: 1 [15200/24000 (63%)]\tLoss: 2.066897\n",
            "Train Epoch: 1 [15400/24000 (64%)]\tLoss: 2.204960\n",
            "Train Epoch: 1 [15600/24000 (65%)]\tLoss: 1.832711\n",
            "Train Epoch: 1 [15800/24000 (66%)]\tLoss: 2.709945\n",
            "Train Epoch: 1 [16000/24000 (67%)]\tLoss: 2.087163\n",
            "Train Epoch: 1 [16200/24000 (68%)]\tLoss: 2.355767\n",
            "Train Epoch: 1 [16400/24000 (68%)]\tLoss: 2.295093\n",
            "Train Epoch: 1 [16600/24000 (69%)]\tLoss: 2.529489\n",
            "Train Epoch: 1 [16800/24000 (70%)]\tLoss: 2.189453\n",
            "Train Epoch: 1 [17000/24000 (71%)]\tLoss: 2.340388\n",
            "Train Epoch: 1 [17200/24000 (72%)]\tLoss: 2.028147\n",
            "Train Epoch: 1 [17400/24000 (72%)]\tLoss: 2.477295\n",
            "Train Epoch: 1 [17600/24000 (73%)]\tLoss: 2.152778\n",
            "Train Epoch: 1 [17800/24000 (74%)]\tLoss: 2.716674\n",
            "Train Epoch: 1 [18000/24000 (75%)]\tLoss: 2.286713\n",
            "Train Epoch: 1 [18200/24000 (76%)]\tLoss: 2.228004\n",
            "Train Epoch: 1 [18400/24000 (77%)]\tLoss: 2.080792\n",
            "Train Epoch: 1 [18600/24000 (78%)]\tLoss: 2.385209\n",
            "Train Epoch: 1 [18800/24000 (78%)]\tLoss: 2.246204\n",
            "Train Epoch: 1 [19000/24000 (79%)]\tLoss: 2.050247\n",
            "Train Epoch: 1 [19200/24000 (80%)]\tLoss: 1.941834\n",
            "Train Epoch: 1 [19400/24000 (81%)]\tLoss: 2.225008\n",
            "Train Epoch: 1 [19600/24000 (82%)]\tLoss: 2.419268\n",
            "Train Epoch: 1 [19800/24000 (82%)]\tLoss: 2.584668\n",
            "Train Epoch: 1 [20000/24000 (83%)]\tLoss: 2.308310\n",
            "Train Epoch: 1 [20200/24000 (84%)]\tLoss: 2.084174\n",
            "Train Epoch: 1 [20400/24000 (85%)]\tLoss: 1.992830\n",
            "Train Epoch: 1 [20600/24000 (86%)]\tLoss: 2.733361\n",
            "Train Epoch: 1 [20800/24000 (87%)]\tLoss: 2.040428\n",
            "Train Epoch: 1 [21000/24000 (88%)]\tLoss: 2.275485\n",
            "Train Epoch: 1 [21200/24000 (88%)]\tLoss: 2.334773\n",
            "Train Epoch: 1 [21400/24000 (89%)]\tLoss: 2.422041\n",
            "Train Epoch: 1 [21600/24000 (90%)]\tLoss: 2.448823\n",
            "Train Epoch: 1 [21800/24000 (91%)]\tLoss: 2.199448\n",
            "Train Epoch: 1 [22000/24000 (92%)]\tLoss: 2.202580\n",
            "Train Epoch: 1 [22200/24000 (92%)]\tLoss: 2.384193\n",
            "Train Epoch: 1 [22400/24000 (93%)]\tLoss: 1.998610\n",
            "Train Epoch: 1 [22600/24000 (94%)]\tLoss: 1.977131\n",
            "Train Epoch: 1 [22800/24000 (95%)]\tLoss: 2.204495\n",
            "Train Epoch: 1 [23000/24000 (96%)]\tLoss: 1.887103\n",
            "Train Epoch: 1 [23200/24000 (97%)]\tLoss: 1.948354\n",
            "Train Epoch: 1 [23400/24000 (98%)]\tLoss: 1.951160\n",
            "Train Epoch: 1 [23600/24000 (98%)]\tLoss: 1.628620\n",
            "Train Epoch: 1 [23800/24000 (99%)]\tLoss: 2.143178\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.0755, Average CER: 0.578300 Average WER: 0.9993\n",
            "\n",
            "Train Epoch: 2 [0/24000 (0%)]\tLoss: 1.983961\n",
            "Train Epoch: 2 [200/24000 (1%)]\tLoss: 1.734300\n",
            "Train Epoch: 2 [400/24000 (2%)]\tLoss: 1.955572\n",
            "Train Epoch: 2 [600/24000 (2%)]\tLoss: 2.039334\n",
            "Train Epoch: 2 [800/24000 (3%)]\tLoss: 2.205929\n",
            "Train Epoch: 2 [1000/24000 (4%)]\tLoss: 2.069931\n",
            "Train Epoch: 2 [1200/24000 (5%)]\tLoss: 2.697856\n",
            "Train Epoch: 2 [1400/24000 (6%)]\tLoss: 1.873681\n",
            "Train Epoch: 2 [1600/24000 (7%)]\tLoss: 2.163377\n",
            "Train Epoch: 2 [1800/24000 (8%)]\tLoss: 1.794128\n",
            "Train Epoch: 2 [2000/24000 (8%)]\tLoss: 3.020179\n",
            "Train Epoch: 2 [2200/24000 (9%)]\tLoss: 1.971223\n",
            "Train Epoch: 2 [2400/24000 (10%)]\tLoss: 2.342767\n",
            "Train Epoch: 2 [2600/24000 (11%)]\tLoss: 2.131378\n",
            "Train Epoch: 2 [2800/24000 (12%)]\tLoss: 2.197392\n",
            "Train Epoch: 2 [3000/24000 (12%)]\tLoss: 2.316895\n",
            "Train Epoch: 2 [3200/24000 (13%)]\tLoss: 2.248061\n",
            "Train Epoch: 2 [3400/24000 (14%)]\tLoss: 1.681503\n",
            "Train Epoch: 2 [3600/24000 (15%)]\tLoss: 2.037071\n",
            "Train Epoch: 2 [3800/24000 (16%)]\tLoss: 1.959766\n",
            "Train Epoch: 2 [4000/24000 (17%)]\tLoss: 1.952226\n",
            "Train Epoch: 2 [4200/24000 (18%)]\tLoss: 2.283918\n",
            "Train Epoch: 2 [4400/24000 (18%)]\tLoss: 1.836733\n",
            "Train Epoch: 2 [4600/24000 (19%)]\tLoss: 2.466387\n",
            "Train Epoch: 2 [4800/24000 (20%)]\tLoss: 2.770917\n",
            "Train Epoch: 2 [5000/24000 (21%)]\tLoss: 2.257665\n",
            "Train Epoch: 2 [5200/24000 (22%)]\tLoss: 2.041125\n",
            "Train Epoch: 2 [5400/24000 (22%)]\tLoss: 2.307521\n",
            "Train Epoch: 2 [5600/24000 (23%)]\tLoss: 2.799222\n",
            "Train Epoch: 2 [5800/24000 (24%)]\tLoss: 1.963993\n",
            "Train Epoch: 2 [6000/24000 (25%)]\tLoss: 1.685134\n",
            "Train Epoch: 2 [6200/24000 (26%)]\tLoss: 2.067508\n",
            "Train Epoch: 2 [6400/24000 (27%)]\tLoss: 2.628541\n",
            "Train Epoch: 2 [6600/24000 (28%)]\tLoss: 2.003590\n",
            "Train Epoch: 2 [6800/24000 (28%)]\tLoss: 2.210391\n",
            "Train Epoch: 2 [7000/24000 (29%)]\tLoss: 2.078666\n",
            "Train Epoch: 2 [7200/24000 (30%)]\tLoss: 2.242143\n",
            "Train Epoch: 2 [7400/24000 (31%)]\tLoss: 2.417446\n",
            "Train Epoch: 2 [7600/24000 (32%)]\tLoss: 2.494332\n",
            "Train Epoch: 2 [7800/24000 (32%)]\tLoss: 2.018343\n",
            "Train Epoch: 2 [8000/24000 (33%)]\tLoss: 2.007260\n",
            "Train Epoch: 2 [8200/24000 (34%)]\tLoss: 2.003810\n",
            "Train Epoch: 2 [8400/24000 (35%)]\tLoss: 1.679143\n",
            "Train Epoch: 2 [8600/24000 (36%)]\tLoss: 2.373494\n",
            "Train Epoch: 2 [8800/24000 (37%)]\tLoss: 1.947869\n",
            "Train Epoch: 2 [9000/24000 (38%)]\tLoss: 1.196320\n",
            "Train Epoch: 2 [9200/24000 (38%)]\tLoss: 2.313565\n",
            "Train Epoch: 2 [9400/24000 (39%)]\tLoss: 2.436618\n",
            "Train Epoch: 2 [9600/24000 (40%)]\tLoss: 1.852965\n",
            "Train Epoch: 2 [9800/24000 (41%)]\tLoss: 2.449528\n",
            "Train Epoch: 2 [10000/24000 (42%)]\tLoss: 1.846969\n",
            "Train Epoch: 2 [10200/24000 (42%)]\tLoss: 2.158333\n",
            "Train Epoch: 2 [10400/24000 (43%)]\tLoss: 2.361964\n",
            "Train Epoch: 2 [10600/24000 (44%)]\tLoss: 2.147151\n",
            "Train Epoch: 2 [10800/24000 (45%)]\tLoss: 1.968795\n",
            "Train Epoch: 2 [11000/24000 (46%)]\tLoss: 1.836509\n",
            "Train Epoch: 2 [11200/24000 (47%)]\tLoss: 1.760677\n",
            "Train Epoch: 2 [11400/24000 (48%)]\tLoss: 1.630897\n",
            "Train Epoch: 2 [11600/24000 (48%)]\tLoss: 2.418226\n",
            "Train Epoch: 2 [11800/24000 (49%)]\tLoss: 2.402198\n",
            "Train Epoch: 2 [12000/24000 (50%)]\tLoss: 1.727286\n",
            "Train Epoch: 2 [12200/24000 (51%)]\tLoss: 2.089913\n",
            "Train Epoch: 2 [12400/24000 (52%)]\tLoss: 1.926486\n",
            "Train Epoch: 2 [12600/24000 (52%)]\tLoss: 2.573719\n",
            "Train Epoch: 2 [12800/24000 (53%)]\tLoss: 1.929675\n",
            "Train Epoch: 2 [13000/24000 (54%)]\tLoss: 1.939425\n",
            "Train Epoch: 2 [13200/24000 (55%)]\tLoss: 2.357930\n",
            "Train Epoch: 2 [13400/24000 (56%)]\tLoss: 1.867152\n",
            "Train Epoch: 2 [13600/24000 (57%)]\tLoss: 2.012772\n",
            "Train Epoch: 2 [13800/24000 (58%)]\tLoss: 1.918383\n",
            "Train Epoch: 2 [14000/24000 (58%)]\tLoss: 1.848213\n",
            "Train Epoch: 2 [14200/24000 (59%)]\tLoss: 1.909686\n",
            "Train Epoch: 2 [14400/24000 (60%)]\tLoss: 2.058241\n",
            "Train Epoch: 2 [14600/24000 (61%)]\tLoss: 2.115442\n",
            "Train Epoch: 2 [14800/24000 (62%)]\tLoss: 2.425330\n",
            "Train Epoch: 2 [15000/24000 (62%)]\tLoss: 2.009577\n",
            "Train Epoch: 2 [15200/24000 (63%)]\tLoss: 1.681213\n",
            "Train Epoch: 2 [15400/24000 (64%)]\tLoss: 1.840068\n",
            "Train Epoch: 2 [15600/24000 (65%)]\tLoss: 1.237915\n",
            "Train Epoch: 2 [15800/24000 (66%)]\tLoss: 2.425758\n",
            "Train Epoch: 2 [16000/24000 (67%)]\tLoss: 1.777826\n",
            "Train Epoch: 2 [16200/24000 (68%)]\tLoss: 1.848583\n",
            "Train Epoch: 2 [16400/24000 (68%)]\tLoss: 2.033223\n",
            "Train Epoch: 2 [16600/24000 (69%)]\tLoss: 2.188010\n",
            "Train Epoch: 2 [16800/24000 (70%)]\tLoss: 1.666543\n",
            "Train Epoch: 2 [17000/24000 (71%)]\tLoss: 1.887018\n",
            "Train Epoch: 2 [17200/24000 (72%)]\tLoss: 1.537360\n",
            "Train Epoch: 2 [17400/24000 (72%)]\tLoss: 2.252687\n",
            "Train Epoch: 2 [17600/24000 (73%)]\tLoss: 1.651337\n",
            "Train Epoch: 2 [17800/24000 (74%)]\tLoss: 2.757186\n",
            "Train Epoch: 2 [18000/24000 (75%)]\tLoss: 1.831941\n",
            "Train Epoch: 2 [18200/24000 (76%)]\tLoss: 1.849137\n",
            "Train Epoch: 2 [18400/24000 (77%)]\tLoss: 1.757514\n",
            "Train Epoch: 2 [18600/24000 (78%)]\tLoss: 1.981327\n",
            "Train Epoch: 2 [18800/24000 (78%)]\tLoss: 1.932249\n",
            "Train Epoch: 2 [19000/24000 (79%)]\tLoss: 1.900274\n",
            "Train Epoch: 2 [19200/24000 (80%)]\tLoss: 1.460302\n",
            "Train Epoch: 2 [19400/24000 (81%)]\tLoss: 2.166831\n",
            "Train Epoch: 2 [19600/24000 (82%)]\tLoss: 2.193609\n",
            "Train Epoch: 2 [19800/24000 (82%)]\tLoss: 2.367130\n",
            "Train Epoch: 2 [20000/24000 (83%)]\tLoss: 2.234874\n",
            "Train Epoch: 2 [20200/24000 (84%)]\tLoss: 1.996474\n",
            "Train Epoch: 2 [20400/24000 (85%)]\tLoss: 2.078741\n",
            "Train Epoch: 2 [20600/24000 (86%)]\tLoss: 2.705420\n",
            "Train Epoch: 2 [20800/24000 (87%)]\tLoss: 1.903999\n",
            "Train Epoch: 2 [21000/24000 (88%)]\tLoss: 2.211738\n",
            "Train Epoch: 2 [21200/24000 (88%)]\tLoss: 2.186163\n",
            "Train Epoch: 2 [21400/24000 (89%)]\tLoss: 1.985690\n",
            "Train Epoch: 2 [21600/24000 (90%)]\tLoss: 1.997288\n",
            "Train Epoch: 2 [21800/24000 (91%)]\tLoss: 2.142945\n",
            "Train Epoch: 2 [22000/24000 (92%)]\tLoss: 1.879331\n",
            "Train Epoch: 2 [22200/24000 (92%)]\tLoss: 2.550256\n",
            "Train Epoch: 2 [22400/24000 (93%)]\tLoss: 1.829295\n",
            "Train Epoch: 2 [22600/24000 (94%)]\tLoss: 1.815923\n",
            "Train Epoch: 2 [22800/24000 (95%)]\tLoss: 1.802480\n",
            "Train Epoch: 2 [23000/24000 (96%)]\tLoss: 1.918426\n",
            "Train Epoch: 2 [23200/24000 (97%)]\tLoss: 1.911534\n",
            "Train Epoch: 2 [23400/24000 (98%)]\tLoss: 1.856820\n",
            "Train Epoch: 2 [23600/24000 (98%)]\tLoss: 1.403563\n",
            "Train Epoch: 2 [23800/24000 (99%)]\tLoss: 1.905600\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.8329, Average CER: 0.499404 Average WER: 1.0172\n",
            "\n",
            "Train Epoch: 3 [0/24000 (0%)]\tLoss: 1.793508\n",
            "Train Epoch: 3 [200/24000 (1%)]\tLoss: 1.494267\n",
            "Train Epoch: 3 [400/24000 (2%)]\tLoss: 1.826378\n",
            "Train Epoch: 3 [600/24000 (2%)]\tLoss: 1.836199\n",
            "Train Epoch: 3 [800/24000 (3%)]\tLoss: 2.042578\n",
            "Train Epoch: 3 [1000/24000 (4%)]\tLoss: 2.143886\n",
            "Train Epoch: 3 [1200/24000 (5%)]\tLoss: 2.440591\n",
            "Train Epoch: 3 [1400/24000 (6%)]\tLoss: 1.610289\n",
            "Train Epoch: 3 [1600/24000 (7%)]\tLoss: 2.092191\n",
            "Train Epoch: 3 [1800/24000 (8%)]\tLoss: 1.640955\n",
            "Train Epoch: 3 [2000/24000 (8%)]\tLoss: 2.865736\n",
            "Train Epoch: 3 [2200/24000 (9%)]\tLoss: 1.805006\n",
            "Train Epoch: 3 [2400/24000 (10%)]\tLoss: 2.392323\n",
            "Train Epoch: 3 [2600/24000 (11%)]\tLoss: 1.845285\n",
            "Train Epoch: 3 [2800/24000 (12%)]\tLoss: 2.125524\n",
            "Train Epoch: 3 [3000/24000 (12%)]\tLoss: 2.055517\n",
            "Train Epoch: 3 [3200/24000 (13%)]\tLoss: 2.252675\n",
            "Train Epoch: 3 [3400/24000 (14%)]\tLoss: 1.565253\n",
            "Train Epoch: 3 [3600/24000 (15%)]\tLoss: 1.827976\n",
            "Train Epoch: 3 [3800/24000 (16%)]\tLoss: 1.680524\n",
            "Train Epoch: 3 [4000/24000 (17%)]\tLoss: 1.756547\n",
            "Train Epoch: 3 [4200/24000 (18%)]\tLoss: 2.165553\n",
            "Train Epoch: 3 [4400/24000 (18%)]\tLoss: 1.203681\n",
            "Train Epoch: 3 [4600/24000 (19%)]\tLoss: 2.189759\n",
            "Train Epoch: 3 [4800/24000 (20%)]\tLoss: 2.651393\n",
            "Train Epoch: 3 [5000/24000 (21%)]\tLoss: 1.980606\n",
            "Train Epoch: 3 [5200/24000 (22%)]\tLoss: 1.920086\n",
            "Train Epoch: 3 [5400/24000 (22%)]\tLoss: 2.211950\n",
            "Train Epoch: 3 [5600/24000 (23%)]\tLoss: 2.749983\n",
            "Train Epoch: 3 [5800/24000 (24%)]\tLoss: 1.801526\n",
            "Train Epoch: 3 [6000/24000 (25%)]\tLoss: 1.468743\n",
            "Train Epoch: 3 [6200/24000 (26%)]\tLoss: 1.897650\n",
            "Train Epoch: 3 [6400/24000 (27%)]\tLoss: 2.491187\n",
            "Train Epoch: 3 [6600/24000 (28%)]\tLoss: 1.784199\n",
            "Train Epoch: 3 [6800/24000 (28%)]\tLoss: 2.122012\n",
            "Train Epoch: 3 [7000/24000 (29%)]\tLoss: 1.994963\n",
            "Train Epoch: 3 [7200/24000 (30%)]\tLoss: 1.649624\n",
            "Train Epoch: 3 [7400/24000 (31%)]\tLoss: 2.106569\n",
            "Train Epoch: 3 [7600/24000 (32%)]\tLoss: 2.678828\n",
            "Train Epoch: 3 [7800/24000 (32%)]\tLoss: 2.156996\n",
            "Train Epoch: 3 [8000/24000 (33%)]\tLoss: 1.834844\n",
            "Train Epoch: 3 [8200/24000 (34%)]\tLoss: 1.634995\n",
            "Train Epoch: 3 [8400/24000 (35%)]\tLoss: 1.548131\n",
            "Train Epoch: 3 [8600/24000 (36%)]\tLoss: 2.256804\n",
            "Train Epoch: 3 [8800/24000 (37%)]\tLoss: 1.964607\n",
            "Train Epoch: 3 [9000/24000 (38%)]\tLoss: 1.124196\n",
            "Train Epoch: 3 [9200/24000 (38%)]\tLoss: 2.256327\n",
            "Train Epoch: 3 [9400/24000 (39%)]\tLoss: 2.435871\n",
            "Train Epoch: 3 [9600/24000 (40%)]\tLoss: 1.726004\n",
            "Train Epoch: 3 [9800/24000 (41%)]\tLoss: 2.158210\n",
            "Train Epoch: 3 [10000/24000 (42%)]\tLoss: 1.658528\n",
            "Train Epoch: 3 [10200/24000 (42%)]\tLoss: 2.472543\n",
            "Train Epoch: 3 [10400/24000 (43%)]\tLoss: 2.362760\n",
            "Train Epoch: 3 [10600/24000 (44%)]\tLoss: 1.988999\n",
            "Train Epoch: 3 [10800/24000 (45%)]\tLoss: 1.935859\n",
            "Train Epoch: 3 [11000/24000 (46%)]\tLoss: 1.648949\n",
            "Train Epoch: 3 [11200/24000 (47%)]\tLoss: 1.627725\n",
            "Train Epoch: 3 [11400/24000 (48%)]\tLoss: 1.426682\n",
            "Train Epoch: 3 [11600/24000 (48%)]\tLoss: 1.809677\n",
            "Train Epoch: 3 [11800/24000 (49%)]\tLoss: 2.330586\n",
            "Train Epoch: 3 [12000/24000 (50%)]\tLoss: 1.522754\n",
            "Train Epoch: 3 [12200/24000 (51%)]\tLoss: 2.034687\n",
            "Train Epoch: 3 [12400/24000 (52%)]\tLoss: 1.874327\n",
            "Train Epoch: 3 [12600/24000 (52%)]\tLoss: 2.366670\n",
            "Train Epoch: 3 [12800/24000 (53%)]\tLoss: 1.775008\n",
            "Train Epoch: 3 [13000/24000 (54%)]\tLoss: 1.858574\n",
            "Train Epoch: 3 [13200/24000 (55%)]\tLoss: 2.184455\n",
            "Train Epoch: 3 [13400/24000 (56%)]\tLoss: 1.939201\n",
            "Train Epoch: 3 [13600/24000 (57%)]\tLoss: 2.139931\n",
            "Train Epoch: 3 [13800/24000 (58%)]\tLoss: 1.861415\n",
            "Train Epoch: 3 [14000/24000 (58%)]\tLoss: 1.665859\n",
            "Train Epoch: 3 [14200/24000 (59%)]\tLoss: 1.726407\n",
            "Train Epoch: 3 [14400/24000 (60%)]\tLoss: 2.049603\n",
            "Train Epoch: 3 [14600/24000 (61%)]\tLoss: 2.172116\n",
            "Train Epoch: 3 [14800/24000 (62%)]\tLoss: 2.345063\n",
            "Train Epoch: 3 [15000/24000 (62%)]\tLoss: 2.088350\n",
            "Train Epoch: 3 [15200/24000 (63%)]\tLoss: 1.257898\n",
            "Train Epoch: 3 [15400/24000 (64%)]\tLoss: 1.515704\n",
            "Train Epoch: 3 [15600/24000 (65%)]\tLoss: 1.151356\n",
            "Train Epoch: 3 [15800/24000 (66%)]\tLoss: 2.154448\n",
            "Train Epoch: 3 [16000/24000 (67%)]\tLoss: 1.649250\n",
            "Train Epoch: 3 [16200/24000 (68%)]\tLoss: 1.749874\n",
            "Train Epoch: 3 [16400/24000 (68%)]\tLoss: 1.930951\n",
            "Train Epoch: 3 [16600/24000 (69%)]\tLoss: 2.145015\n",
            "Train Epoch: 3 [16800/24000 (70%)]\tLoss: 1.423572\n",
            "Train Epoch: 3 [17000/24000 (71%)]\tLoss: 1.699248\n",
            "Train Epoch: 3 [17200/24000 (72%)]\tLoss: 1.483956\n",
            "Train Epoch: 3 [17400/24000 (72%)]\tLoss: 2.229832\n",
            "Train Epoch: 3 [17600/24000 (73%)]\tLoss: 1.413801\n",
            "Train Epoch: 3 [17800/24000 (74%)]\tLoss: 2.374126\n",
            "Train Epoch: 3 [18000/24000 (75%)]\tLoss: 1.961072\n",
            "Train Epoch: 3 [18200/24000 (76%)]\tLoss: 1.863918\n",
            "Train Epoch: 3 [18400/24000 (77%)]\tLoss: 1.731679\n",
            "Train Epoch: 3 [18600/24000 (78%)]\tLoss: 1.776533\n",
            "Train Epoch: 3 [18800/24000 (78%)]\tLoss: 1.964659\n",
            "Train Epoch: 3 [19000/24000 (79%)]\tLoss: 1.735079\n",
            "Train Epoch: 3 [19200/24000 (80%)]\tLoss: 1.348852\n",
            "Train Epoch: 3 [19400/24000 (81%)]\tLoss: 1.786672\n",
            "Train Epoch: 3 [19600/24000 (82%)]\tLoss: 1.903009\n",
            "Train Epoch: 3 [19800/24000 (82%)]\tLoss: 2.436564\n",
            "Train Epoch: 3 [20000/24000 (83%)]\tLoss: 1.890763\n",
            "Train Epoch: 3 [20200/24000 (84%)]\tLoss: 1.993357\n",
            "Train Epoch: 3 [20400/24000 (85%)]\tLoss: 1.579673\n",
            "Train Epoch: 3 [20600/24000 (86%)]\tLoss: 2.543726\n",
            "Train Epoch: 3 [20800/24000 (87%)]\tLoss: 1.744337\n",
            "Train Epoch: 3 [21000/24000 (88%)]\tLoss: 1.910075\n",
            "Train Epoch: 3 [21200/24000 (88%)]\tLoss: 2.117423\n",
            "Train Epoch: 3 [21400/24000 (89%)]\tLoss: 1.834370\n",
            "Train Epoch: 3 [21600/24000 (90%)]\tLoss: 1.720882\n",
            "Train Epoch: 3 [21800/24000 (91%)]\tLoss: 1.952537\n",
            "Train Epoch: 3 [22000/24000 (92%)]\tLoss: 2.008780\n",
            "Train Epoch: 3 [22200/24000 (92%)]\tLoss: 2.444981\n",
            "Train Epoch: 3 [22400/24000 (93%)]\tLoss: 1.764971\n",
            "Train Epoch: 3 [22600/24000 (94%)]\tLoss: 1.889546\n",
            "Train Epoch: 3 [22800/24000 (95%)]\tLoss: 1.903687\n",
            "Train Epoch: 3 [23000/24000 (96%)]\tLoss: 1.654917\n",
            "Train Epoch: 3 [23200/24000 (97%)]\tLoss: 1.517157\n",
            "Train Epoch: 3 [23400/24000 (98%)]\tLoss: 1.975629\n",
            "Train Epoch: 3 [23600/24000 (98%)]\tLoss: 1.104744\n",
            "Train Epoch: 3 [23800/24000 (99%)]\tLoss: 2.029994\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.7404, Average CER: 0.477846 Average WER: 0.9800\n",
            "\n",
            "Train Epoch: 4 [0/24000 (0%)]\tLoss: 1.843656\n",
            "Train Epoch: 4 [200/24000 (1%)]\tLoss: 1.563730\n",
            "Train Epoch: 4 [400/24000 (2%)]\tLoss: 1.767919\n",
            "Train Epoch: 4 [600/24000 (2%)]\tLoss: 1.765125\n",
            "Train Epoch: 4 [800/24000 (3%)]\tLoss: 1.957427\n",
            "Train Epoch: 4 [1000/24000 (4%)]\tLoss: 1.827828\n",
            "Train Epoch: 4 [1200/24000 (5%)]\tLoss: 2.535367\n",
            "Train Epoch: 4 [1400/24000 (6%)]\tLoss: 1.418422\n",
            "Train Epoch: 4 [1600/24000 (7%)]\tLoss: 1.708071\n",
            "Train Epoch: 4 [1800/24000 (8%)]\tLoss: 1.693112\n",
            "Train Epoch: 4 [2000/24000 (8%)]\tLoss: 2.660466\n",
            "Train Epoch: 4 [2200/24000 (9%)]\tLoss: 1.672621\n",
            "Train Epoch: 4 [2400/24000 (10%)]\tLoss: 2.236619\n",
            "Train Epoch: 4 [2600/24000 (11%)]\tLoss: 1.719547\n",
            "Train Epoch: 4 [2800/24000 (12%)]\tLoss: 1.846262\n",
            "Train Epoch: 4 [3000/24000 (12%)]\tLoss: 2.132674\n",
            "Train Epoch: 4 [3200/24000 (13%)]\tLoss: 2.251194\n",
            "Train Epoch: 4 [3400/24000 (14%)]\tLoss: 1.342120\n",
            "Train Epoch: 4 [3600/24000 (15%)]\tLoss: 1.676476\n",
            "Train Epoch: 4 [3800/24000 (16%)]\tLoss: 1.707123\n",
            "Train Epoch: 4 [4000/24000 (17%)]\tLoss: 1.754293\n",
            "Train Epoch: 4 [4200/24000 (18%)]\tLoss: 2.044760\n",
            "Train Epoch: 4 [4400/24000 (18%)]\tLoss: 1.135452\n",
            "Train Epoch: 4 [4600/24000 (19%)]\tLoss: 2.207059\n",
            "Train Epoch: 4 [4800/24000 (20%)]\tLoss: 2.790716\n",
            "Train Epoch: 4 [5000/24000 (21%)]\tLoss: 1.949196\n",
            "Train Epoch: 4 [5200/24000 (22%)]\tLoss: 1.639338\n",
            "Train Epoch: 4 [5400/24000 (22%)]\tLoss: 1.948045\n",
            "Train Epoch: 4 [5600/24000 (23%)]\tLoss: 2.737091\n",
            "Train Epoch: 4 [5800/24000 (24%)]\tLoss: 1.783054\n",
            "Train Epoch: 4 [6000/24000 (25%)]\tLoss: 1.696284\n",
            "Train Epoch: 4 [6200/24000 (26%)]\tLoss: 1.864413\n",
            "Train Epoch: 4 [6400/24000 (27%)]\tLoss: 2.397125\n",
            "Train Epoch: 4 [6600/24000 (28%)]\tLoss: 1.604420\n",
            "Train Epoch: 4 [6800/24000 (28%)]\tLoss: 1.923929\n",
            "Train Epoch: 4 [7000/24000 (29%)]\tLoss: 1.829018\n",
            "Train Epoch: 4 [7200/24000 (30%)]\tLoss: 1.569310\n",
            "Train Epoch: 4 [7400/24000 (31%)]\tLoss: 2.109752\n",
            "Train Epoch: 4 [7600/24000 (32%)]\tLoss: 2.148226\n",
            "Train Epoch: 4 [7800/24000 (32%)]\tLoss: 1.763819\n",
            "Train Epoch: 4 [8000/24000 (33%)]\tLoss: 1.845788\n",
            "Train Epoch: 4 [8200/24000 (34%)]\tLoss: 1.621731\n",
            "Train Epoch: 4 [8400/24000 (35%)]\tLoss: 1.437588\n",
            "Train Epoch: 4 [8600/24000 (36%)]\tLoss: 2.198290\n",
            "Train Epoch: 4 [8800/24000 (37%)]\tLoss: 1.832498\n",
            "Train Epoch: 4 [9000/24000 (38%)]\tLoss: 0.917922\n",
            "Train Epoch: 4 [9200/24000 (38%)]\tLoss: 2.339040\n",
            "Train Epoch: 4 [9400/24000 (39%)]\tLoss: 2.385534\n",
            "Train Epoch: 4 [9600/24000 (40%)]\tLoss: 1.608137\n",
            "Train Epoch: 4 [9800/24000 (41%)]\tLoss: 2.373608\n",
            "Train Epoch: 4 [10000/24000 (42%)]\tLoss: 1.795289\n",
            "Train Epoch: 4 [10200/24000 (42%)]\tLoss: 2.440981\n",
            "Train Epoch: 4 [10400/24000 (43%)]\tLoss: 2.113573\n",
            "Train Epoch: 4 [10600/24000 (44%)]\tLoss: 1.822472\n",
            "Train Epoch: 4 [10800/24000 (45%)]\tLoss: 1.802993\n",
            "Train Epoch: 4 [11000/24000 (46%)]\tLoss: 1.568233\n",
            "Train Epoch: 4 [11200/24000 (47%)]\tLoss: 1.643352\n",
            "Train Epoch: 4 [11400/24000 (48%)]\tLoss: 1.397327\n",
            "Train Epoch: 4 [11600/24000 (48%)]\tLoss: 1.926978\n",
            "Train Epoch: 4 [11800/24000 (49%)]\tLoss: 2.334160\n",
            "Train Epoch: 4 [12000/24000 (50%)]\tLoss: 1.508342\n",
            "Train Epoch: 4 [12200/24000 (51%)]\tLoss: 2.038443\n",
            "Train Epoch: 4 [12400/24000 (52%)]\tLoss: 1.865138\n",
            "Train Epoch: 4 [12600/24000 (52%)]\tLoss: 2.120372\n",
            "Train Epoch: 4 [12800/24000 (53%)]\tLoss: 1.871719\n",
            "Train Epoch: 4 [13000/24000 (54%)]\tLoss: 1.756337\n",
            "Train Epoch: 4 [13200/24000 (55%)]\tLoss: 2.076064\n",
            "Train Epoch: 4 [13400/24000 (56%)]\tLoss: 1.656639\n",
            "Train Epoch: 4 [13600/24000 (57%)]\tLoss: 1.963554\n",
            "Train Epoch: 4 [13800/24000 (58%)]\tLoss: 1.800749\n",
            "Train Epoch: 4 [14000/24000 (58%)]\tLoss: 1.732282\n",
            "Train Epoch: 4 [14200/24000 (59%)]\tLoss: 1.576837\n",
            "Train Epoch: 4 [14400/24000 (60%)]\tLoss: 1.883686\n",
            "Train Epoch: 4 [14600/24000 (61%)]\tLoss: 2.105935\n",
            "Train Epoch: 4 [14800/24000 (62%)]\tLoss: 2.318471\n",
            "Train Epoch: 4 [15000/24000 (62%)]\tLoss: 2.003516\n",
            "Train Epoch: 4 [15200/24000 (63%)]\tLoss: 1.157864\n",
            "Train Epoch: 4 [15400/24000 (64%)]\tLoss: 1.509809\n",
            "Train Epoch: 4 [15600/24000 (65%)]\tLoss: 1.070779\n",
            "Train Epoch: 4 [15800/24000 (66%)]\tLoss: 2.212560\n",
            "Train Epoch: 4 [16000/24000 (67%)]\tLoss: 1.581859\n",
            "Train Epoch: 4 [16200/24000 (68%)]\tLoss: 1.654725\n",
            "Train Epoch: 4 [16400/24000 (68%)]\tLoss: 1.952985\n",
            "Train Epoch: 4 [16600/24000 (69%)]\tLoss: 1.988873\n",
            "Train Epoch: 4 [16800/24000 (70%)]\tLoss: 1.543944\n",
            "Train Epoch: 4 [17000/24000 (71%)]\tLoss: 1.611180\n",
            "Train Epoch: 4 [17200/24000 (72%)]\tLoss: 1.341123\n",
            "Train Epoch: 4 [17400/24000 (72%)]\tLoss: 2.158538\n",
            "Train Epoch: 4 [17600/24000 (73%)]\tLoss: 1.476286\n",
            "Train Epoch: 4 [17800/24000 (74%)]\tLoss: 2.410921\n",
            "Train Epoch: 4 [18000/24000 (75%)]\tLoss: 1.724606\n",
            "Train Epoch: 4 [18200/24000 (76%)]\tLoss: 1.738103\n",
            "Train Epoch: 4 [18400/24000 (77%)]\tLoss: 1.720789\n",
            "Train Epoch: 4 [18600/24000 (78%)]\tLoss: 1.961569\n",
            "Train Epoch: 4 [18800/24000 (78%)]\tLoss: 1.743544\n",
            "Train Epoch: 4 [19000/24000 (79%)]\tLoss: 1.675022\n",
            "Train Epoch: 4 [19200/24000 (80%)]\tLoss: 1.405946\n",
            "Train Epoch: 4 [19400/24000 (81%)]\tLoss: 1.566578\n",
            "Train Epoch: 4 [19600/24000 (82%)]\tLoss: 1.730799\n",
            "Train Epoch: 4 [19800/24000 (82%)]\tLoss: 2.329359\n",
            "Train Epoch: 4 [20000/24000 (83%)]\tLoss: 1.818443\n",
            "Train Epoch: 4 [20200/24000 (84%)]\tLoss: 1.990930\n",
            "Train Epoch: 4 [20400/24000 (85%)]\tLoss: 1.711797\n",
            "Train Epoch: 4 [20600/24000 (86%)]\tLoss: 2.637148\n",
            "Train Epoch: 4 [20800/24000 (87%)]\tLoss: 1.596099\n",
            "Train Epoch: 4 [21000/24000 (88%)]\tLoss: 1.861248\n",
            "Train Epoch: 4 [21200/24000 (88%)]\tLoss: 2.098014\n",
            "Train Epoch: 4 [21400/24000 (89%)]\tLoss: 1.768091\n",
            "Train Epoch: 4 [21600/24000 (90%)]\tLoss: 1.924428\n",
            "Train Epoch: 4 [21800/24000 (91%)]\tLoss: 1.868234\n",
            "Train Epoch: 4 [22000/24000 (92%)]\tLoss: 1.814957\n",
            "Train Epoch: 4 [22200/24000 (92%)]\tLoss: 2.228967\n",
            "Train Epoch: 4 [22400/24000 (93%)]\tLoss: 1.709147\n",
            "Train Epoch: 4 [22600/24000 (94%)]\tLoss: 1.618910\n",
            "Train Epoch: 4 [22800/24000 (95%)]\tLoss: 1.844775\n",
            "Train Epoch: 4 [23000/24000 (96%)]\tLoss: 1.547806\n",
            "Train Epoch: 4 [23200/24000 (97%)]\tLoss: 1.521686\n",
            "Train Epoch: 4 [23400/24000 (98%)]\tLoss: 1.644257\n",
            "Train Epoch: 4 [23600/24000 (98%)]\tLoss: 1.178849\n",
            "Train Epoch: 4 [23800/24000 (99%)]\tLoss: 1.777735\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.6835, Average CER: 0.458694 Average WER: 0.9651\n",
            "\n",
            "Train Epoch: 5 [0/24000 (0%)]\tLoss: 1.599218\n",
            "Train Epoch: 5 [200/24000 (1%)]\tLoss: 1.357018\n",
            "Train Epoch: 5 [400/24000 (2%)]\tLoss: 1.647014\n",
            "Train Epoch: 5 [600/24000 (2%)]\tLoss: 1.788291\n",
            "Train Epoch: 5 [800/24000 (3%)]\tLoss: 2.136640\n",
            "Train Epoch: 5 [1000/24000 (4%)]\tLoss: 1.695113\n",
            "Train Epoch: 5 [1200/24000 (5%)]\tLoss: 2.524386\n",
            "Train Epoch: 5 [1400/24000 (6%)]\tLoss: 1.434681\n",
            "Train Epoch: 5 [1600/24000 (7%)]\tLoss: 1.696929\n",
            "Train Epoch: 5 [1800/24000 (8%)]\tLoss: 1.495466\n",
            "Train Epoch: 5 [2000/24000 (8%)]\tLoss: 2.380988\n",
            "Train Epoch: 5 [2200/24000 (9%)]\tLoss: 1.674545\n",
            "Train Epoch: 5 [2400/24000 (10%)]\tLoss: 2.258880\n",
            "Train Epoch: 5 [2600/24000 (11%)]\tLoss: 1.670015\n",
            "Train Epoch: 5 [2800/24000 (12%)]\tLoss: 2.063861\n",
            "Train Epoch: 5 [3000/24000 (12%)]\tLoss: 1.870238\n",
            "Train Epoch: 5 [3200/24000 (13%)]\tLoss: 2.044081\n",
            "Train Epoch: 5 [3400/24000 (14%)]\tLoss: 1.266997\n",
            "Train Epoch: 5 [3600/24000 (15%)]\tLoss: 1.698884\n",
            "Train Epoch: 5 [3800/24000 (16%)]\tLoss: 1.849084\n",
            "Train Epoch: 5 [4000/24000 (17%)]\tLoss: 1.718337\n",
            "Train Epoch: 5 [4200/24000 (18%)]\tLoss: 2.008107\n",
            "Train Epoch: 5 [4400/24000 (18%)]\tLoss: 1.264873\n",
            "Train Epoch: 5 [4600/24000 (19%)]\tLoss: 2.317337\n",
            "Train Epoch: 5 [4800/24000 (20%)]\tLoss: 2.675670\n",
            "Train Epoch: 5 [5000/24000 (21%)]\tLoss: 1.903293\n",
            "Train Epoch: 5 [5200/24000 (22%)]\tLoss: 1.605839\n",
            "Train Epoch: 5 [5400/24000 (22%)]\tLoss: 1.739437\n",
            "Train Epoch: 5 [5600/24000 (23%)]\tLoss: 2.772452\n",
            "Train Epoch: 5 [5800/24000 (24%)]\tLoss: 1.752273\n",
            "Train Epoch: 5 [6000/24000 (25%)]\tLoss: 1.614469\n",
            "Train Epoch: 5 [6200/24000 (26%)]\tLoss: 2.012217\n",
            "Train Epoch: 5 [6400/24000 (27%)]\tLoss: 2.549417\n",
            "Train Epoch: 5 [6600/24000 (28%)]\tLoss: 1.716805\n",
            "Train Epoch: 5 [6800/24000 (28%)]\tLoss: 1.924978\n",
            "Train Epoch: 5 [7000/24000 (29%)]\tLoss: 1.888699\n",
            "Train Epoch: 5 [7200/24000 (30%)]\tLoss: 1.518127\n",
            "Train Epoch: 5 [7400/24000 (31%)]\tLoss: 2.155560\n",
            "Train Epoch: 5 [7600/24000 (32%)]\tLoss: 2.208251\n",
            "Train Epoch: 5 [7800/24000 (32%)]\tLoss: 2.142110\n",
            "Train Epoch: 5 [8000/24000 (33%)]\tLoss: 1.721058\n",
            "Train Epoch: 5 [8200/24000 (34%)]\tLoss: 1.418536\n",
            "Train Epoch: 5 [8400/24000 (35%)]\tLoss: 1.415200\n",
            "Train Epoch: 5 [8600/24000 (36%)]\tLoss: 2.249812\n",
            "Train Epoch: 5 [8800/24000 (37%)]\tLoss: 1.798802\n",
            "Train Epoch: 5 [9000/24000 (38%)]\tLoss: 0.798876\n",
            "Train Epoch: 5 [9200/24000 (38%)]\tLoss: 2.231266\n",
            "Train Epoch: 5 [9400/24000 (39%)]\tLoss: 2.365760\n",
            "Train Epoch: 5 [9600/24000 (40%)]\tLoss: 1.607927\n",
            "Train Epoch: 5 [9800/24000 (41%)]\tLoss: 2.418641\n",
            "Train Epoch: 5 [10000/24000 (42%)]\tLoss: 1.588679\n",
            "Train Epoch: 5 [10200/24000 (42%)]\tLoss: 2.175137\n",
            "Train Epoch: 5 [10400/24000 (43%)]\tLoss: 1.983185\n",
            "Train Epoch: 5 [10600/24000 (44%)]\tLoss: 1.829158\n",
            "Train Epoch: 5 [10800/24000 (45%)]\tLoss: 1.618196\n",
            "Train Epoch: 5 [11000/24000 (46%)]\tLoss: 1.558909\n",
            "Train Epoch: 5 [11200/24000 (47%)]\tLoss: 1.496528\n",
            "Train Epoch: 5 [11400/24000 (48%)]\tLoss: 1.284546\n",
            "Train Epoch: 5 [11600/24000 (48%)]\tLoss: 1.921600\n",
            "Train Epoch: 5 [11800/24000 (49%)]\tLoss: 2.340908\n",
            "Train Epoch: 5 [12000/24000 (50%)]\tLoss: 1.451047\n",
            "Train Epoch: 5 [12200/24000 (51%)]\tLoss: 2.064124\n",
            "Train Epoch: 5 [12400/24000 (52%)]\tLoss: 1.546587\n",
            "Train Epoch: 5 [12600/24000 (52%)]\tLoss: 2.136548\n",
            "Train Epoch: 5 [12800/24000 (53%)]\tLoss: 1.738214\n",
            "Train Epoch: 5 [13000/24000 (54%)]\tLoss: 1.643007\n",
            "Train Epoch: 5 [13200/24000 (55%)]\tLoss: 2.075969\n",
            "Train Epoch: 5 [13400/24000 (56%)]\tLoss: 1.717010\n",
            "Train Epoch: 5 [13600/24000 (57%)]\tLoss: 1.870395\n",
            "Train Epoch: 5 [13800/24000 (58%)]\tLoss: 1.663995\n",
            "Train Epoch: 5 [14000/24000 (58%)]\tLoss: 1.517711\n",
            "Train Epoch: 5 [14200/24000 (59%)]\tLoss: 1.509021\n",
            "Train Epoch: 5 [14400/24000 (60%)]\tLoss: 1.872624\n",
            "Train Epoch: 5 [14600/24000 (61%)]\tLoss: 1.970181\n",
            "Train Epoch: 5 [14800/24000 (62%)]\tLoss: 2.167046\n",
            "Train Epoch: 5 [15000/24000 (62%)]\tLoss: 1.987473\n",
            "Train Epoch: 5 [15200/24000 (63%)]\tLoss: 1.177017\n",
            "Train Epoch: 5 [15400/24000 (64%)]\tLoss: 1.479970\n",
            "Train Epoch: 5 [15600/24000 (65%)]\tLoss: 0.927442\n",
            "Train Epoch: 5 [15800/24000 (66%)]\tLoss: 2.181371\n",
            "Train Epoch: 5 [16000/24000 (67%)]\tLoss: 1.581556\n",
            "Train Epoch: 5 [16200/24000 (68%)]\tLoss: 1.670745\n",
            "Train Epoch: 5 [16400/24000 (68%)]\tLoss: 1.947944\n",
            "Train Epoch: 5 [16600/24000 (69%)]\tLoss: 2.008030\n",
            "Train Epoch: 5 [16800/24000 (70%)]\tLoss: 1.367561\n",
            "Train Epoch: 5 [17000/24000 (71%)]\tLoss: 1.627246\n",
            "Train Epoch: 5 [17200/24000 (72%)]\tLoss: 1.219872\n",
            "Train Epoch: 5 [17400/24000 (72%)]\tLoss: 2.003088\n",
            "Train Epoch: 5 [17600/24000 (73%)]\tLoss: 1.324335\n",
            "Train Epoch: 5 [17800/24000 (74%)]\tLoss: 2.362825\n",
            "Train Epoch: 5 [18000/24000 (75%)]\tLoss: 1.555914\n",
            "Train Epoch: 5 [18200/24000 (76%)]\tLoss: 1.635289\n",
            "Train Epoch: 5 [18400/24000 (77%)]\tLoss: 1.569982\n",
            "Train Epoch: 5 [18600/24000 (78%)]\tLoss: 1.446629\n",
            "Train Epoch: 5 [18800/24000 (78%)]\tLoss: 1.599196\n",
            "Train Epoch: 5 [19000/24000 (79%)]\tLoss: 1.748647\n",
            "Train Epoch: 5 [19200/24000 (80%)]\tLoss: 1.257853\n",
            "Train Epoch: 5 [19400/24000 (81%)]\tLoss: 1.780619\n",
            "Train Epoch: 5 [19600/24000 (82%)]\tLoss: 1.584101\n",
            "Train Epoch: 5 [19800/24000 (82%)]\tLoss: 2.331295\n",
            "Train Epoch: 5 [20000/24000 (83%)]\tLoss: 1.959683\n",
            "Train Epoch: 5 [20200/24000 (84%)]\tLoss: 1.634161\n",
            "Train Epoch: 5 [20400/24000 (85%)]\tLoss: 1.436039\n",
            "Train Epoch: 5 [20600/24000 (86%)]\tLoss: 2.611927\n",
            "Train Epoch: 5 [20800/24000 (87%)]\tLoss: 1.573676\n",
            "Train Epoch: 5 [21000/24000 (88%)]\tLoss: 1.933266\n",
            "Train Epoch: 5 [21200/24000 (88%)]\tLoss: 1.925509\n",
            "Train Epoch: 5 [21400/24000 (89%)]\tLoss: 1.664007\n",
            "Train Epoch: 5 [21600/24000 (90%)]\tLoss: 1.601175\n",
            "Train Epoch: 5 [21800/24000 (91%)]\tLoss: 1.899797\n",
            "Train Epoch: 5 [22000/24000 (92%)]\tLoss: 1.905861\n",
            "Train Epoch: 5 [22200/24000 (92%)]\tLoss: 2.445738\n",
            "Train Epoch: 5 [22400/24000 (93%)]\tLoss: 1.492252\n",
            "Train Epoch: 5 [22600/24000 (94%)]\tLoss: 1.544939\n",
            "Train Epoch: 5 [22800/24000 (95%)]\tLoss: 1.691754\n",
            "Train Epoch: 5 [23000/24000 (96%)]\tLoss: 1.314432\n",
            "Train Epoch: 5 [23200/24000 (97%)]\tLoss: 1.409769\n",
            "Train Epoch: 5 [23400/24000 (98%)]\tLoss: 1.930839\n",
            "Train Epoch: 5 [23600/24000 (98%)]\tLoss: 1.110983\n",
            "Train Epoch: 5 [23800/24000 (99%)]\tLoss: 1.699094\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.6006, Average CER: 0.435096 Average WER: 0.9155\n",
            "\n",
            "Train Epoch: 6 [0/24000 (0%)]\tLoss: 1.780006\n",
            "Train Epoch: 6 [200/24000 (1%)]\tLoss: 1.197645\n",
            "Train Epoch: 6 [400/24000 (2%)]\tLoss: 1.392724\n"
          ]
        }
      ],
      "source": [
        "def main(learning_rate=5e-4, batch_size=2, epochs=13,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\",\n",
        "        experiment=Experiment(api_key='dummy_key', disabled=True)):\n",
        "  global data\n",
        "  train_dataset = LIBRISPEECH_CUSTOM(train_df)\n",
        "  test_dataset = LIBRISPEECH_CUSTOM(test_df)\n",
        "\n",
        "  hparams = {\n",
        "          \"n_cnn_layers\": 3,\n",
        "          \"n_rnn_layers\": 5,\n",
        "          \"rnn_dim\": 512,\n",
        "          \"n_class\": 29,\n",
        "          \"n_feats\": 128,\n",
        "          \"stride\":2,\n",
        "          \"dropout\": 0.2,\n",
        "          \"learning_rate\": learning_rate,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"epochs\": epochs\n",
        "      }\n",
        "\n",
        "  experiment.log_parameters(hparams)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  torch.manual_seed(7)\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "  train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=hparams['batch_size'],\n",
        "                                  shuffle=False,\n",
        "                                  collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                  **kwargs)\n",
        "  print(test_dataset.__getitem__(40))\n",
        "  # data_iter = iter(data_tmp)\n",
        "  #batch = next(data_iter)\n",
        "  #print(batch['image'])\n",
        "\n",
        "  test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "  model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "  model = nn.DataParallel(model)\n",
        "  print(model)\n",
        "  print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "  optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "  criterion = nn.CTCLoss(blank=28).to(device)\n",
        "  scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "  iter_meter = IterMeter()\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
        "      test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma5-B5LAJbId"
      },
      "outputs": [],
      "source": [
        "experiment.end()\n",
        "# for i in range(df.shape[0]):\n",
        "#   print(i)\n",
        "#   if i != 758:\n",
        "#     a = df['text'][i].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAqo4v0Pz6uS"
      },
      "outputs": [],
      "source": [
        "#model.get_last_lr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5tew0cTydUL"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.sate_dict(), 'model_saved.pth')\n",
        "#print('model saved at model_saved.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmFHx2XEzNvj"
      },
      "outputs": [],
      "source": [
        "#state_dict = torch.load('model_saved.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}